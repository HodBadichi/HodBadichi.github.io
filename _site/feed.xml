<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://0.0.0.0:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4001/" rel="alternate" type="text/html" /><updated>2025-06-08T00:41:31+03:00</updated><id>http://0.0.0.0:4001/feed.xml</id><subtitle>Writing about performance, networking, and the things that puzzle me</subtitle><author><name>Hod Badichi</name></author><entry><title type="html">Understanding Memory Level Parallelism Through Binary Search</title><link href="http://0.0.0.0:4001/blog/2025/06/01/memory-parallelism-modern-cpus-copy" rel="alternate" type="text/html" title="Understanding Memory Level Parallelism Through Binary Search" /><published>2025-06-01T00:00:00+03:00</published><updated>2025-06-01T00:00:00+03:00</updated><id>http://0.0.0.0:4001/blog/2025/06/01/memory-parallelism-modern-cpus%20copy</id><content type="html" xml:base="http://0.0.0.0:4001/blog/2025/06/01/memory-parallelism-modern-cpus-copy">&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; You can find the code used for this analysis in the following GitHub repository: &lt;a href=&quot;https://github.com/HodBadichi/Posts-Code/tree/main/19122024&quot;&gt;Posts-Code/19122024&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Beyond binary search’s $O(log n)$ complexity lies a story of CPU optimization. Modern processors don’t wait for memory they predict, prefetch, and parallelize memory operations, turning what seems like a simple algorithm into a showcase of memory level parallelism.&lt;/p&gt;

&lt;p&gt;Let’s start with a standard binary search implementation:&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;binary_search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ll use an array big enough to exceed our caches of $3.73$[GB] ($100$[m] integers) and perform $10$[m] lookups. Using Intel TMA (Top-down Microarchitecture Analysis), we can see the performance bottlenecks:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    74,870,673,648      TOPDOWN.SLOTS                    #     20.1 %  tma_backend_bound      
                                                  #     42.7 %  tma_bad_speculation    
                                                  #     33.7 %  tma_frontend_bound     
                                                  #      3.5 %  tma_retiring  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Only 20% of the time is spent on backend operations (including memory stalls). The majority of time (42.7%) is spent on bad speculation, which makes sense given the 50/50 branch prediction scenario in binary search.&lt;/p&gt;

&lt;h2 id=&quot;measuring-memory-level-parallelism&quot;&gt;Measuring Memory Level Parallelism&lt;/h2&gt;

&lt;p&gt;Unlike other performance metrics, MLP cannot be directly measured through PMU counters. Instead, best practices typically rely on either full-system simulators or analytical/partial modeling approaches to quantify MLP characteristics.&lt;/p&gt;

&lt;p&gt;One practical approach to estimate average MLP is through the following perf events:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l1d_pend_miss.pending&lt;/code&gt;: Total number of pending L1 data cache misses&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l1d_pend_miss.pending_cycles&lt;/code&gt;: Total cycles with pending L1 data cache misses&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These metrics allow us to calculate the average MLP specifically for L1 data cache misses, which provides insight into how effectively our code utilizes memory-level parallelism at the L1 cache level:&lt;/p&gt;

\[MLP_{avg} = \frac{l1d\_pend\_miss.pending}{l1d\_pend\_miss.pending}\]

&lt;p&gt;The calculated $MLP_{avg}$ represents the average number of concurrent L1 cache misses. This value is fundamentally limited by the number of MSHRs available in the L1 data cache, which determines how many outstanding cache misses the processor can track simultaneously.&lt;/p&gt;

&lt;p&gt;Analyzing our binary search implementation yields the following metrics:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    98,278,263,019      l1d_pend_miss.pending

    11,219,972,459      l1d_pend_miss.pending_cycles 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Calculating the average MLP:&lt;/p&gt;

&lt;p&gt;$MLP_{avg} = \frac{98,278,263,019}{11,219,972,459} \approx 8.8$&lt;/p&gt;

&lt;p&gt;This result highlights a crucial aspect of modern CPU performance analysis: while our binary search algorithm appears to be inherently serial, the actual MLP measurement shows significant parallelism. This discrepancy occurs because modern CPUs employ speculation techniques that can overlap memory operations, even in seemingly sequential code.&lt;/p&gt;

&lt;p&gt;The observed MLP of 8.76 is significantly higher than the expected value of 1, which can be explained by two types of out-of-order execution:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Function-level overlap: The CPU can execute multiple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary_search()&lt;/code&gt; calls concurrently, as these are independent operations that don’t require speculation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iteration-level overlap: Within a single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary_search()&lt;/code&gt; call, the CPU overlap memory operations across different iterations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key distinction is that function-level overlap is deterministic (the CPU knows these operations are independent), while iteration-level overlap requires speculation (the CPU must predict which path the binary search will take).&lt;/p&gt;

&lt;p&gt;To measure the impact of function-level overlap, we can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtscp&lt;/code&gt; instruction as a serialization barrier, forcing the CPU to complete one query before starting the next. Running our benchmark with this modification yields:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    109,103,656,601      l1d_pend_miss.pending                                                 
     13,004,667,591      l1d_pend_miss.pending_cycles   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Calculating the MLP for this version:
\(MLP_{avg} = \frac{109,103,656,601}{13,004,667,591} \approx 8.39\)&lt;/p&gt;

&lt;p&gt;This result is particularly interesting: despite using a serialization barrier, the MLP only decreased by about 5% from our original measurement of 8.76. This suggests that function-level overlap contributes less to our observed MLP than we might have expected, indicating that most of the parallelism comes from iteration-level overlap within each binary search operation.&lt;/p&gt;

&lt;p&gt;Running a branchless version we will be able to verify the MLP indeed comes from speculation:&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;binary_search_branchless&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and indeed we get  MLP of 1.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    14,443,856,192      l1d_pend_miss.pending                                                 
    14,418,507,045      l1d_pend_miss.pending_cycles  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;mlp-limitations&quot;&gt;MLP limitations&lt;/h3&gt;

&lt;p&gt;MLP is constrained by two main factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Hardware limitations:
    &lt;ul&gt;
      &lt;li&gt;Reorder Buffer (ROB) size: Determines how many instructions can be in-flight&lt;/li&gt;
      &lt;li&gt;Number of MSHRs (Miss Status Holding Registers): Limits concurrent cache misses&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Program characteristics:
    &lt;ul&gt;
      &lt;li&gt;Load clustering: How memory accesses are distributed&lt;/li&gt;
      &lt;li&gt;Out-of-order opportunities: Sequential dependencies (like pointer chasing) limit parallel execution&lt;/li&gt;
      &lt;li&gt;Speculation opportunities: Whether the code allows for effective branch prediction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our binary search example, we observed an MLP of 8. This value reveals several insights:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The MSHR limit isn’t the bottleneck (Intel Sapphire Rapids supports 16 MSHRs)&lt;/li&gt;
  &lt;li&gt;The observed MLP of 8 suggests the limiting factors are:
    &lt;ul&gt;
      &lt;li&gt;ROB size constraints&lt;/li&gt;
      &lt;li&gt;Limited speculation depth beyond L1 cache&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Honestly, without a detailed microarchitecture simulator, it’s pretty hard to say exactly why our MLP isn’t maxing out the MSHRs. A full-on simulator would probably be able to figure it out, though.&lt;/p&gt;</content><author><name>Hod Badichi</name></author><category term="cpu-architecture" /><category term="performance" /><category term="memory" /><category term="hardware" /><summary type="html">Beyond binary search&apos;s $O(log n)$ complexity lies a story of CPU optimization. Modern processors don&apos;t wait for memory they predict, prefetch, and parallelize memory operations, turning what seems like a simple algorithm into a showcase of memory level parallelism.</summary></entry></feed>